{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Z3 Baseline Model Template Applied**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objectives\n",
        "\n",
        "* Establish a baseline binary classifier for the cybersecurity intrusion dataset using logistic regression.\n",
        "* Document a reproducible modelling workflow that follows the team notebook template structure.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* `data/processed/cybersecurity_intrusion_data_eda.csv`: feature-engineered dataset prepared during previous steps of the project.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Fitted `LogisticRegression` pipeline with preprocessing steps and tuned hyperparameters.\n",
        "* Evaluation metrics (classification report and confusion matrix) for the held-out test set.\n",
        "* Ranked logistic regression coefficients to highlight the most influential features.\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* The workflow maintains template sections and focuses exclusively on the logistic regression baseline requested for Zone 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chdir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import libraries and load the modelling dataset\n",
        "\n",
        "This section collects all external dependencies and loads the processed cybersecurity intrusion data so the remaining steps can focus on modelling tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from feature_engine.imputation import MeanMedianImputer, CategoricalImputer\n",
        "from feature_engine.encoding import OneHotEncoder\n",
        "\n",
        "# Visual style for plots\n",
        "sns.set_style(\"whitegrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineered cybersecurity dataset\n",
        "df_clf = pd.read_csv('data/processed/cybersecurity_intrusion_data_eda.csv')\n",
        "print(f'Dataset shape: {df_clf.shape}')\n",
        "df_clf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare features and create train/test splits\n",
        "\n",
        "Columns that were replaced by engineered versions are removed before splitting. A stratified split preserves the attack rate across training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_to_drop = ['session_id', 'network_packet_size', 'session_duration', 'ip_reputation_score']\n",
        "\n",
        "df_model = df_clf.drop(columns=columns_to_drop)\n",
        "X = df_model.drop(columns=['attack_detected'])\n",
        "y = df_model['attack_detected']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=101, stratify=y\n",
        ")\n",
        "\n",
        "print('* Train set:', X_train.shape, y_train.shape)\n",
        "print('* Test set:', X_test.shape, y_test.shape)\n",
        "y.value_counts(normalize=True).rename('overall_attack_rate')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Section 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the logistic regression pipeline and tune hyperparameters\n",
        "\n",
        "The pipeline handles imputation, categorical encoding and feature scaling before fitting a logistic regression model. A `GridSearchCV` explores a compact hyperparameter grid for different regularisation strengths while keeping the workflow limited to logistic regression as requested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logistic_pipeline = Pipeline([\n",
        "    ('median', MeanMedianImputer(imputation_method='median', variables=['ip_reputation_score_log'])),\n",
        "    ('categorical_imputer', CategoricalImputer(imputation_method='frequent', variables=['browser_type'])),\n",
        "    ('onehot', OneHotEncoder(variables=['browser_type', 'protocol_type', 'encryption_used'], drop_last=True)),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LogisticRegression(solver='saga', max_iter=2000, n_jobs=-1, random_state=101))\n",
        "])\n",
        "\n",
        "param_grid = [\n",
        "    {'model__penalty': ['l1'], 'model__C': [0.1, 1.0, 10.0]},\n",
        "    {'model__penalty': ['l2'], 'model__C': [0.1, 1.0, 10.0]},\n",
        "    {'model__penalty': ['elasticnet'], 'model__C': [0.1, 1.0, 10.0], 'model__l1_ratio': [0.0, 0.5, 1.0]},\n",
        "]\n",
        "\n",
        "logistic_search = GridSearchCV(\n",
        "    estimator=logistic_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    refit=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "logistic_search.fit(X_train, y_train)\n",
        "\n",
        "print('Best parameters:', logistic_search.best_params_)\n",
        "print(f\"Best mean CV F1-score: {logistic_search.best_score_:.3f}\")\n",
        "\n",
        "cv_results = (\n",
        "    pd.DataFrame(logistic_search.cv_results_)[\n",
        "        ['mean_test_score', 'std_test_score', 'param_model__penalty', 'param_model__C', 'param_model__l1_ratio']\n",
        "    ]\n",
        "    .sort_values('mean_test_score', ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "cv_results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Section 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the tuned baseline on the test set\n",
        "\n",
        "Model diagnostics focus on standard classification metrics and a confusion matrix so stakeholders can compare performance against future experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_pipeline = logistic_search.best_estimator_\n",
        "\n",
        "test_predictions = best_pipeline.predict(X_test)\n",
        "print('Classification report (test set):')\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 4))\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_test, test_predictions,\n",
        "    display_labels=['No Attack', 'Attack'],\n",
        "    colorbar=False,\n",
        "    cmap='Blues',\n",
        "    ax=ax\n",
        ")\n",
        "ax.set_title('Confusion matrix (test set)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Section 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpret logistic regression coefficients\n",
        "\n",
        "Coefficients provide directional insight into how each engineered feature influences the log-odds of detecting an attack after scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_engineering_steps = Pipeline(best_pipeline.steps[:-2])\n",
        "feature_matrix = feature_engineering_steps.transform(X_train)\n",
        "feature_names = feature_matrix.columns\n",
        "\n",
        "coef_series = pd.Series(best_pipeline.named_steps['model'].coef_[0], index=feature_names, name='coefficient')\n",
        "coef_df = (\n",
        "    coef_series.to_frame()\n",
        "    .assign(absolute_coefficient=lambda df_: df_['coefficient'].abs())\n",
        "    .sort_values('absolute_coefficient', ascending=False)\n",
        ")\n",
        "coef_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Conclusions and Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Logistic regression with elastic net regularisation offers a solid baseline for the intrusion detection task and achieves competitive recall on the attack class.\n",
        "* Future iterations can compare additional algorithms or extend the feature engineering pipeline, using this notebook as a controlled reference point."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12 (default, Nov  7 2022, 16:45:55) \n[GCC 9.4.0]"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}